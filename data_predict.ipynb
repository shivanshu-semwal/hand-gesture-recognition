{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3b0ad03-ee72-4569-979e-635a80cf906a",
   "metadata": {},
   "source": [
    "# Data Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32381427-e6f5-4e35-9da1-daecf9cf3b21",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afa289a4-9228-4c52-8a9e-e77edc4d1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d4f886-2d93-471a-909b-a927a0054732",
   "metadata": {},
   "source": [
    "#### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95004ff6-17c7-44db-9fe0-df0b4e120d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = None\n",
    "temp_image = 'temp.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0278d712-ab05-400b-9f64-7d9b11cdd68c",
   "metadata": {},
   "source": [
    "### Resize Image\n",
    "\n",
    "Used to resize the image given as input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9ad0622-2d28-4354-bc7e-583306a05c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resizeImage(imageName):\n",
    "    basewidth = 100\n",
    "    img = Image.open(imageName)\n",
    "    wpercent = (basewidth/float(img.size[0]))\n",
    "    hsize = int((float(img.size[1])*float(wpercent)))\n",
    "    img = img.resize((basewidth, hsize), Image.ANTIALIAS)\n",
    "    img.save(imageName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2814ed39-6763-441d-b092-ed874bf8bc14",
   "metadata": {},
   "source": [
    "## Running Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e8a72d7-65d5-4e66-8997-b24d9ac0d5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_avg(image, aWeight):\n",
    "    global bg\n",
    "    # initialize the background\n",
    "    if bg is None:\n",
    "        bg = image.copy().astype(\"float\")\n",
    "        return\n",
    "    # compute weighted average, accumulate it and update the background\n",
    "    cv2.accumulateWeighted(image, bg, aWeight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab182e7e-4969-4848-8a50-a75fdb1b640e",
   "metadata": {},
   "source": [
    "## Segimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b03573c-9153-4c4e-8f3c-b12ff80399da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(image, threshold=25):\n",
    "    global bg\n",
    "    # find the absolute difference between background and current frame\n",
    "    diff = cv2.absdiff(bg.astype(\"uint8\"), image)\n",
    "\n",
    "    # threshold the diff image so that we get the foreground\n",
    "    thresholded = cv2.threshold(diff,\n",
    "                                threshold,\n",
    "                                255,\n",
    "                                cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # get the contours in the thresholded image\n",
    "    (cnts, _) = cv2.findContours(thresholded.copy(),\n",
    "                                 cv2.RETR_EXTERNAL,\n",
    "                                 cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # return None, if no contours detected\n",
    "    if len(cnts) == 0:\n",
    "        return\n",
    "    else:\n",
    "        # based on contour area, get the maximum contour which is the hand\n",
    "        segmented = max(cnts, key=cv2.contourArea)\n",
    "        return (thresholded, segmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0451bc4-7ee2-46c5-b22b-1e77947ecef4",
   "metadata": {},
   "source": [
    "## Getting Predicted Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8c59766-1eb1-4c82-b16b-af6e813208b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictedClass():\n",
    "    # read the image\n",
    "    image = cv2.imread(temp_image)\n",
    "    # convert to greyscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # change to numpy array\n",
    "    gray_image = gray_image.reshape(89, 100, 1)\n",
    "    # change to numpy array with shape (1, 89, 100, 1) so model can receive it\n",
    "    gray_image = np.array([gray_image])\n",
    "    # predict the image\n",
    "    prediction = model.predict(gray_image)\n",
    "    # return a numpy array with all values for layers\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63518f98-7500-42c4-93b5-98e4f71cccba",
   "metadata": {},
   "source": [
    "## Displaying Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a619a8cb-419b-4e3b-a3e4-b147a0bf4fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showStatistics(prediction):\n",
    "    gestures = ['fist', 'palm', 'swing', 'ok']\n",
    "    n = len(gestures)\n",
    "    x = 30\n",
    "    y = 30\n",
    "    height = (n+3)*y\n",
    "    width = 500\n",
    "    textImage = np.zeros((height, width, 3), np.uint8)\n",
    "    for i in range(0, len(gestures)):\n",
    "        cv2.putText(textImage, \n",
    "                    gestures[i] + ' : ' + f\"{prediction[0][i]:.2f}\" , \n",
    "                    (x, y),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1,\n",
    "                    (255, 255, 255),\n",
    "                    2)\n",
    "        y = y + 30\n",
    "        \n",
    "    predicted_gesture = gestures[np.argmax(prediction)]\n",
    "    sum = 0.00\n",
    "    for i in prediction[0]:\n",
    "        sum += i\n",
    "    confidence = (np.amax(prediction) /  sum) * 100\n",
    "    \n",
    "    cv2.putText(textImage, \n",
    "            \"Gesture: \" + predicted_gesture, \n",
    "            (x, y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (255, 255, 255),\n",
    "            2)\n",
    "    y += 30\n",
    "    cv2.putText(textImage, \n",
    "            \"Confidence: \" + str(confidence) + \"%\", \n",
    "            (x, y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (255, 255, 255),\n",
    "            2)\n",
    "    cv2.imshow(\"Statistics \", textImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6024781f-6e0a-49f5-bed1-80724279ac1f",
   "metadata": {},
   "source": [
    "#### (BUG) GPU memory overflow\n",
    "> this is a workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "885048ff-82d4-4129-9af3-cd532c4726dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "method1 = False\n",
    "\n",
    "if method1:\n",
    "    import os\n",
    "    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    gpu_options.allow_growth = True\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "#\n",
    "\n",
    "method2 = False\n",
    "\n",
    "if method2:\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "    tf.compat.v1.keras.backend.set_session(\n",
    "        tf.compat.v1.Session(config=config))\n",
    "\n",
    "# \n",
    "\n",
    "method3 = False\n",
    "\n",
    "if method3:\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a247faac-4b27-4e4f-8170-1e4871e6c96f",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "769a2239-195a-4f4c-9b00-3be00cba76a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-18 09:17:59.797799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-18 09:17:59.874956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-18 09:17:59.875137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-18 09:17:59.876584: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-18 09:17:59.877161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-18 09:17:59.877319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-18 09:17:59.877458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-18 09:18:01.038023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-18 09:18:01.038216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-18 09:18:01.038371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-18 09:18:01.038487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2241 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "model = models.load_model('./TrainedModel/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8650b9-b60f-4485-bdf4-fe8c4b07984c",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b6a5d3b-39ba-4c1b-990a-25c2c59f6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weight for running average\n",
    "aWeight = 0.5\n",
    "\n",
    "# region of interest (ROI) coordinates\n",
    "top, right, bottom, left = 10, 350, 225, 590\n",
    "\n",
    "# initialize num of frames\n",
    "num_frames = 0\n",
    "start_recording = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bed23344-0c8e-4f00-abc9-c72898bd10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the reference to the webcam\n",
    "camera = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e339d54-702e-4e39-87d2-4c0166026275",
   "metadata": {},
   "source": [
    "Press `s` to start recording, and press `q` to quit.\n",
    "Please wait for some time till black background is formed.\n",
    "\n",
    "In case you get camera not found error, intialize the camera again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce79b521-a127-4a44-929a-a1b95851241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep looping, until interrupted\n",
    "while(True):\n",
    "    # get the current frame\n",
    "    grabbed, frame = camera.read()\n",
    "\n",
    "    if grabbed:\n",
    "        # resize the frame\n",
    "        frame = imutils.resize(frame, width=700)\n",
    "\n",
    "        # flip the frame so that it is not the mirror view\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # clone the frame\n",
    "        clone = frame.copy()\n",
    "\n",
    "        # get the ROI\n",
    "        roi = frame[top:bottom, right:left]\n",
    "\n",
    "        # convert the roi to grayscale and blur it\n",
    "        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "\n",
    "        # to get the background, keep looking till a threshold is reached\n",
    "        # so that our running average model gets calibrated\n",
    "        if num_frames < 30:\n",
    "            run_avg(gray, aWeight)\n",
    "        else:\n",
    "            # segment the hand region\n",
    "            hand = segment(gray)\n",
    "\n",
    "            # check whether hand region is segmented\n",
    "            if hand is not None:\n",
    "                # if yes, unpack the thresholded image and\n",
    "                # segmented region\n",
    "                (thresholded, segmented) = hand\n",
    "\n",
    "                # draw the segmented region and display the frame\n",
    "                cv2.drawContours(\n",
    "                    clone, [segmented + (right, top)], -1, (0, 0, 255))\n",
    "                if start_recording:\n",
    "                    cv2.imwrite(temp_image, thresholded)\n",
    "                    resizeImage(temp_image)\n",
    "                    # predictedClass, confidence = getPredictedClass()\n",
    "                    prediction = getPredictedClass()\n",
    "                    showStatistics(prediction)\n",
    "\n",
    "                cv2.imshow(\"Thesholded\", thresholded)\n",
    "\n",
    "        # draw the segmented hand\n",
    "        cv2.rectangle(clone, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "\n",
    "        # increment the number of frames\n",
    "        num_frames += 1\n",
    "\n",
    "        # display the frame with segmented hand\n",
    "        cv2.imshow(\"Video Feed\", clone)\n",
    "\n",
    "        # observe the keypress by the user\n",
    "        keypress = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # if the user pressed \"q\", then stop looping\n",
    "        if keypress == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "        if keypress == ord(\"s\"):\n",
    "            start_recording = True\n",
    "    else:\n",
    "        print(\"Error, Please check your camera\")\n",
    "        print(camera)\n",
    "        break\n",
    "\n",
    "# relaease the resources\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed0a4452-2008-48e6-94bd-f56ad858a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove temporary image file\n",
    "import os\n",
    "os.remove(temp_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:shiv]",
   "language": "python",
   "name": "conda-env-shiv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
